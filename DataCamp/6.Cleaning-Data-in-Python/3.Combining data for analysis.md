&copy; Copyright for Shuang Wu 2017<br>
Cite from the DataCamp website<br>
**Do not cite this notebook in any situation**<br>
Persenoal reference only, **do not copy** the code or others
- [Concatenating data](#concatenating-data)
    - [Concatenating data](#concatenating-data)
    - [Combining rows of data](#combining-rows-of-data)
    - [Combining columns of data](#combining-columns-of-data)
- [Finding and concatenating data](#finding-and-concatenating-data)
    - [Finding and concatenating data](#finding-and-concatenating-data)
    - [Finding files that match a pattern](#finding-files-that-match-a-pattern)
    - [Iterating and concatenating all matches](#iterating-and-concatenating-all-matches)
- [Merge data](#merge-data)
    - [Merge data](#merge-data)
    - [1-to-1 data merge](#1-to-1-data-merge)
    - [Many-to-1 data merge](#many-to-1-data-merge)
    - [Many-to-many data merge](#many-to-many-data-merge)


# Concatenating data

## Concatenating data

* Combining data
    * Data may not always come in 1 huge file
        * 5m row dataset may be broken into 5 separate datasets
        * Easier to store and share
        * May have new data for each day
    *  Important to be able to combine then clean, or vice versa
*  Concatenation
    *  combine two ro more data set into one dataset
    *  can use pandas concat

## Combining rows of data

* Concatenate these DFs together

```python {.input}
row_concat = pd.concat([uber1, uber2, uber3])
print(row_concat.shape)
print(row_concat.head())
```
Output:<br>
<center>(297, 4)</center>

Index | Date/Time | Lat | Lon | Base
------|-----------|-----|-----|-----
0 | 4/1/2014 0:11:00 | 40.7690 | -73.9549 | B02512
1 | 4/1/2014 0:17:00 | 40.7267 | -74.0345 | B02512
2 | 4/1/2014 0:21:00 | 40.7316 | -73.9873 | B02512
3 | 4/1/2014 0:28:00 | 40.7588 | -73.9776 | B02512
4 | 4/1/2014 0:33:00 | 40.7594 | -73.9722 | B02512

## Combining columns of data

* Column-wise concatenation of data as stitching data together from the sides instead of the top and bottom
* Use _pd.concate()_ w/ argument _axis=1_
    * the default, _axis=0_ is for row-wise concatenation

```python {.input}
ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)
print(ebola_tidy.shape)
print(ebola_tidy.head())
```
Output:<br>
<center>(1952, 6)</center>

Index | Date | Day | status_country | counts | status | country
------|------|-----|----------------|--------|--------|--------
0 | 1/5/2015 | 289 | Cases_Guinea | 2776.0 | Cases | Guinea
1 | 1/4/2015 | 288 | Cases_Guinea | 2775.0 | Cases | Guinea
2 | 1/3/2015 | 287 | Cases_Guinea | 2769.0 | Cases | Guinea
3 | 1/2/2015 | 286 | Cases_Guinea | NaN | Cases | Guinea
4 | 12/31/2014 | 284 | Cases_Guinea | 2730.0 | Cases | Guinea

# Finding and concatenating data

## Finding and concatenating data

* Concatenating many files
    * Leverage Python's features w/ data cleaning in pandas
    * In order to concatenate DFs:
        * They must be in a list
        * Can individually load if there are a few datasets
        * But what if there are thousands
    * Solution: glob function to find files based on a pattern
* Globbing
    * Pattern matching for file names
    * Wildcars: *, ?
        * Any csv file: *.csv
        * Any single character: file_?.csv
    * returns a list of file names
    * Can use this list to load DF use pd
* The plan
    * Load files from globbing into pandas
    * Add the DFs into a list
    * Concatenate multiple datasets ar once
* Find and concatenate

## Finding files that match a pattern

* Using the _glob_ module to find all csv files in the workspace
* The _glob_ module has a function called _glob_ that takes a pattern and returns a list of the files in the working directory that match that pattern
* E.g., if pattern is _part\_+single digit number+.csv_, can write the pattern as _'part\_?.csv'_
* Can find all _.csv_ w/ _'*.csv'_
* The _?_ wildcard represents any 1 character, and the _*_ wildcard represents any number of characters

```python {.input}
import glob
import pandas as pd
pattern = '*.csv'
csv_files = glob.glob(pattern)
print(csv_files)
csv2 = pd.read_csv(csv_files[1])
print(csv2.head())
```
Output:<br>
<center>   ['uber-raw-data-2014_06.csv', 'uber-raw-data-2014_04.csv', 'uber-raw-data-2014_05.csv']</center>

Index | Date/Time | Lat | Lon | Base
------|-----------|-----|-----|-----
0 | 4/1/2014 0:11:00 | 40.7690 | -73.9549 | B02512
1 | 4/1/2014 0:17:00 | 40.7267 | -74.0345 | B02512
2 | 4/1/2014 0:21:00 | 40.7316 | -73.9873 | B02512
3 | 4/1/2014 0:28:00 | 40.7588 | -73.9776 | B02512
4 | 4/1/2014 0:33:00 | 40.7594 | -73.9722 | B02512

## Iterating and concatenating all matches

* Can load all the files into a list of DFs that can then be concatenated
* Start w/ an empty list, use a _for_ loop to iterate through each of the filenames, read each filename into a DF, and then append it to the list
* Concatenate the list using _pd.concate()_

```python {.input}
frames = []
for csv in csv_files:
    df = pd.read_csv(csv)
    frames.append(df)
uber = pd.concat(frames)
print(uber.shape)
print(uber.head())
```
Output:<br>
<center>(297, 4)</center>

Index | Date/Time | Lat | Lon | Base
------|-----------|-----|-----|-----
0 | 6/1/2014 0:00:00 | 40.7293 | -73.9920 | B02512
1 | 6/1/2014 0:01:00 | 40.7131 | -74.0097 | B02512
2 | 6/1/2014 0:04:00 | 40.3461 | -74.6610 | B02512
3 | 6/1/2014 0:04:00 | 40.7555 | -73.9833 | B02512
4 | 6/1/2014 0:07:00 | 40.6880 | -74.1831 | B02512

# Merge data

## Merge data

* Combining data
    * Concatenation is not the only way data can be combined
* Merging data
    * similar to joining tables in SQL
    * Combine disparate datasets based on common columns
* Types of merges
    * one-to-one
    * Many-to-one / one-to-many
    * many-to-many
    * All use the same function
    * only difference is the DFs merging

## 1-to-1 data merge

* Merging data allows combine disparate datasets into a single dataset to do more complex analysis

```python {.input}
o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')
print(o2o)
```
Output:<br>
Index | name | lat | long | ident | site | dated
------|------|-----|------|-------|------|------
0 | DR-1 | -49.85 | -128.57 | 619 | DR-1 | 1927-02-08
1 | DR-3 | -47.15 | -126.72 | 734 | DR-3 | 1939-01-07
2 | MSK-4 | -48.87 | -123.40 | 837 | MSK-4 | 1932-01-14

## Many-to-1 data merge

* In many-to-one (or one-to-many) merge, one of the values will be duplicated and recycled in the output
* One of the keys in the merge is not unique
```python {.input}
m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')
print(m2o)
```
Output:<br>
Index | name | lat | long | ident | site | dated
------|------|-----|------|-------|------|------
0 | DR-1 | -49.85 | -128.57 | 619 | DR-1 | 1927-02-08
1 | DR-1 | -49.85 | -128.57 | 622 | DR-1 | 1927-02-10
2 | DR-1 | -49.85 | -128.57 | 844 | DR-1 | 1932-03-22
3 | DR-3 | -47.15 | -126.72 | 734 | DR-3 | 1939-01-07
4 | DR-3 | -47.15 | -126.72 | 735 | DR-3 | 1930-01-12
5 | DR-3 | -47.15 | -126.72 | 751 | DR-3 | 1930-02-26
6 | DR-3 | -47.15 | -126.72 | 752 | DR-3 | NaN
7 | MSK-4 | -48.87 | -123.40 | 837 | MSK-4 | 1932-01-14

## Many-to-many data merge

* Final merging scenario occurs when both DFs do not have unique keys for a merge
* For each duplicated key, every pairwise combination will be created

```python {.input}
m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')
m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')
print(m2m.head(20))
```
Output:<br>
Index | name | lat | long | ident | site | dated | taken | person | quant | reading | 
------|------|-----|------|-------|------|-------|-------|--------|-------|---------|-
0 | DR-1 | -49.85 | -128.57 | 619 | DR-1 | 1927-02-08 | 619 | dyer | rad | 9.82 | 
1 | DR-1 | -49.85 | -128.57 | 619 | DR-1 | 1927-02-08 | 619 | dyer | sal | 0.13 | 
2 | DR-1 | -49.85 | -128.57 | 622 | DR-1 | 1927-02-10 | 622 | dyer | rad | 7.80 | 
3 | DR-1 | -49.85 | -128.57 | 622 | DR-1 | 1927-02-10 | 622 | dyer | sal | 0.09 | 
4 | DR-1 | -49.85 | -128.57 | 844 | DR-1 | 1932-03-22 | 844 | roe | rad | 11.25 | 
5 | DR-3 | -47.15 | -126.72 | 734 | DR-3 | 1939-01-07 | 734 | pb | rad | 8.41 | 
6 | DR-3 | -47.15 | -126.72 | 734 | DR-3 | 1939-01-07 | 734 | lake | sal | 0.05 | 
7 | DR-3 | -47.15 | -126.72 | 734 | DR-3 | 1939-01-07 | 734 | pb | temp | -21.50 | 
8 | DR-3 | -47.15 | -126.72 | 735 | DR-3 | 1930-01-12 | 735 | pb | rad | 7.22 | 
9 | DR-3 | -47.15 | -126.72 | 735 | DR-3 | 1930-01-12 | 735 | NaN | sal | 0.06 | 
10 | DR-3 | -47.15 | -126.72 | 735 | DR-3 | 1930-01-12 | 735 | NaN | temp | -26.00 | 
11 | DR-3 | -47.15 | -126.72 | 751 | DR-3 | 1930-02-26 | 751 | pb | rad | 4.35 | 
12 | DR-3 | -47.15 | -126.72 | 751 | DR-3 | 1930-02-26 | 751 | pb | temp | -18.50 | 
13 | DR-3 | -47.15 | -126.72 | 751 | DR-3 | 1930-02-26 | 751 | lake | sal | 0.10 | 
14 | DR-3 | -47.15 | -126.72 | 752 | DR-3 | NaN | 752 | lake | rad | 2.19 | 
15 | DR-3 | -47.15 | -126.72 | 752 | DR-3 | NaN | 752 | lake | sal | 0.09 | 
16 | DR-3 | -47.15 | -126.72 | 752 | DR-3 | NaN | 752 | lake | temp | -16.00 | 
17 | DR-3 | -47.15 | -126.72 | 752 | DR-3 | NaN | 752 | roe | sal | 41.60 | 
18 | MSK-4 | -48.87 | -123.40 | 837 | MSK-4 | 1932-01-14 | 837 | lake | rad | 1.46 | 
19 | MSK-4 | -48.87 | -123.40 | 837 | MSK-4 | 1932-01-14 | 837 | lake | sal | 0.21 | 
